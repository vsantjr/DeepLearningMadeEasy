{"cells":[{"cell_type":"markdown","source":["## **Deep Learning Made Easy**\n","\n","----\n","\n","**Important:** The code of this notebook was developed by <a href=\"https://machinelearningmastery.com/the-attention-mechanism-from-scratch/\">Stefania Cristina</a> published in the *Machine Learning Mastery* site. The text was based on the above post and some others as indicated below. A few modifications have been done by <a href=\"https://www.linkedin.com/in/valdivino-alexandre-de-santiago-j%C3%BAnior-103109206/?locale=en_US\">Valdivino Alexandre de Santiago Júnior</a>. It is a notebook to explain the attention mechanism.\n","\n","<br>\n","\n","**If you wish to cite this material, please do so by actually citing the other authors, especially <a href=\"https://machinelearningmastery.com/the-attention-mechanism-from-scratch/\">Stefania Cristina's</a> post.**\n","\n","\n","<br>\n","\n","To run this notebook in Colab, use the link below:\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vsantjr/DeepLearningMadeEasy/blob/master/Attention_Mechanism.ipynb)"],"metadata":{"id":"goL2EJiJLipT"}},{"cell_type":"markdown","source":["## Seq2Seq Model\n","----\n","\n","From: https://www.kdnuggets.com/2021/01/attention-mechanism-deep-learning-explained.html\n","\n","\n","The seq2seq model is normally composed of an encoder-decoder architecture, where the encoder processes the input sequence and encodes/compresses the information into a context vector (or “thought vector”) **of fixed length**. This representation is anticipated to be a good summary of the **complete input sequence**. The decoder is then initialized with this context vector, using which it starts producing the transformed or translated output.\n","\n","A critical disadvantage of this **fixed-length context vector design is the inability of the system to retain longer sequences**. Often it has forgotten the earlier elements of the input sequence once it has processed the complete sequence. **The attention mechanism was created to resolve this problem of long dependencies**.\n","\n","Thus the main idea of the attention mechanism is that each time the model predicts an output word, it only uses parts of the input where the most relevant information is concentrated instead of the entire sequence. **In simpler words, it only pays attention to some input words (not the entire sequence of words)**.\n","\n","\n","\n"],"metadata":{"id":"3Qzy9n441DgQ"}},{"cell_type":"markdown","source":["## Example\n","----\n","\n"],"metadata":{"id":"MA3Kxo8J3MUu"}},{"cell_type":"markdown","source":["Let us consider this sentence:\n","\n","<br>\n","\n","\"The cat is sleeping on the couch\". \n","\n","<br>\n","\n","And the translation to Portuguese:\n","\n","<br>\n","\n","\"O gato está dormindo no sofá\".\n","\n","<br>\n","\n","<img src=\"https://drive.google.com/uc?id=1sKqU02obQ8Y4_680RBXmxwjB2p4Q2ro3\" alt=\"Drawing\" width=\"300\"/>\n","\n","<br>\n","\n","Then when predicting \"gato\" it is clear that this name is the result of the word \"cat\" present in the input English sentence regardless of the rest of the sentence. Thus, we say that while predicting \"gato\", we **pay more attention** to the word \"cat\" in the input sentence.\n","\n"],"metadata":{"id":"GY0R0XX71fbE"}},{"cell_type":"markdown","source":["## The Attention Mechanism\n","----\n","\n","From: https://machinelearningmastery.com/the-attention-mechanism-from-scratch/\n","\n","See also: https://docs.google.com/presentation/d/1ZXFIhYczos679r70Yu8vV9uO6B1J0ztzeDxbnBxD1S0/edit#slide=id.g31364026ad_3_2\n","\n","The attention mechanism proposed by <a href=\"https://arxiv.org/abs/1409.0473\">Bahdanau et al. (2015)</a> is divided into step-by-step computations of the alignment scores, the weights, and the context vector as described below:\n","\n","\n","\n","*   **Alignment scores**. The alignment model takes the encoded hidden states, $h_i$, and the previous decoder output, $s_{t-1}$ , to compute a score, $e_{t,i}$, that indicates how well the elements of the input sequence align with the current output at the position, $t$. The alignment model is represented by a function, $f(.)$, which can be implemented by a feedforward neural network: \n","\n","$$\n","e_{t,i} = f(s_{t-1}, h_i);\n","$$ \n","*   **Weights**. The weights, $\\alpha_{t,i}$, are computed by applying a softmax operation to the previously computed alignment scores:\n","\n","$$\n","\\alpha_{t,i} = softmax(e_{t,i});\n","$$\n","\n","*  **Context vector**. A unique context vector, $c_t$, is fed into the decoder at each time step. It is computed by a weighted sum of all, $T$, encoder hidden states:\n","\n","$$\n","c_t = \\sum_{i=1}^{T} \\alpha_{t,i} \\cdot h_i\n","$$\n","\n","They implemented an RNN for both the encoder and decoder.\n","\n","<br>\n","\n","<br>\n","\n","<img src=\"https://drive.google.com/uc?id=1e7YMnFvPwnDTdD_ePHWlgEx5XHSBtn6i\" alt=\"Drawing\" width=\"1000\"/>\n","\n","\n","\n","Source: <a href=\"https://arxiv.org/abs/1409.0473\">Bahdanau et al. (2015) </a> and <a href=\"https://docs.google.com/presentation/d/1ZXFIhYczos679r70Yu8vV9uO6B1J0ztzeDxbnBxD1S0/edit#slide=id.g31364026ad_3_2\">Beyer (2022)</a>."],"metadata":{"id":"noVgDBN_JEF1"}},{"cell_type":"markdown","source":["## The General Attention Mechanism\n","---\n","\n","From: https://machinelearningmastery.com/the-attention-mechanism-from-scratch/\n","\n","See also: https://docs.google.com/presentation/d/1ZXFIhYczos679r70Yu8vV9uO6B1J0ztzeDxbnBxD1S0/edit#slide=id.g31364026ad_3_2\n","\n","\n","The general attention mechanism makes use of three main components, namely the queries, $Q$, the keys, $K$, and the values, $V$. \n","\n","If you compare these three components to the attention mechanism as proposed by <a href=\"https://arxiv.org/abs/1409.0473\">Bahdanau et al. (2015)</a>, then the **query** would be analogous to the **previous decoder output, $s_{t-1}$**, while the **values** would be analogous to the **encoded inputs, $h_i$**. In the Bahdanau attention mechanism, **the keys and values are the same vector**.\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"GczRq92Uu-3t"}},{"cell_type":"markdown","source":["## Self-Attention (SA)\n","----\n","\n","From: https://blogs.oracle.com/ai-and-datascience/post/multi-head-self-attention-in-nlp\n","\n","We can also explain the general attention mechanism as follows. See its mathematical representation below.\n","\n","<br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1IXRH1QBjN3_WDmeO6pkUvytKXIH0Nns-\" alt=\"Drawing\" width=\"600\"/>\n","\n","Source: <a href=\"https://blogs.oracle.com/ai-and-datascience/post/multi-head-self-attention-in-nlp\">Praphul Singh</a>.\n","\n","<br>\n","\n","So, $X$ is the **input word sequence**, and we calculate three values from that which is $Q$, $K$ and $V$. The task is to find the important words from the **keys** for the **query** word. This is done by passing the query and key to a mathematical function (usually matrix multiplication followed by softmax). The resulting context vector for $Q$ is the multiplication of the probability vector obtained by the softmax with the **value**.\n","\n","When the **query**, **key**, and **value** are all generated from the same input sequence $X$, the general attention mechanism is called **self-attention**.\n","\n","\n","\n"],"metadata":{"id":"-wGEyTJVRqgt"}},{"cell_type":"markdown","source":["## Multi-Head Self-Attention (MSA)\n","----\n","\n","From: https://blogs.oracle.com/ai-and-datascience/post/multi-head-self-attention-in-nlp\n","\n","See also:  https://docs.google.com/presentation/d/1ZXFIhYczos679r70Yu8vV9uO6B1J0ztzeDxbnBxD1S0/edit#slide=id.g31364026ad_3_2\n","\n","**Multi-head** is just doing the same thing discussed earlier by different heads (brains?). The aim here is to combine the knowledge explored by multiple heads or agents instead of doing it by one, as in the traditional case.\n","\n","Mathematically, it relates to attending to not only the different words of the sentence, but to different segments of the words, too. The words vectors are divided into a fixed number ($H$, number of heads) of chunks, and then self-attention is applied to the corresponding chunks, resulting in $H$ context vectors for each word. The final context vector is obtained by concatenating all those $H$ context vectors.\n","\n","<br>\n","\n","<img src=\"https://drive.google.com/uc?id=1kRiTpmdEU1U1EsxrVRMWdTcmeIOgb06U\" alt=\"Drawing\" width=\"450\"/>\n","\n","Source: <a href=\"https://blogs.oracle.com/ai-and-datascience/post/multi-head-self-attention-in-nlp\">Praphul Singh</a>.\n","\n","<br>\n","\n","Above is a visualisation of the outputs upon using two heads. We can see that if the **query** word is \"it\", the first head focuses more on the words \"the animal\", and the second head focuses more on the word \"tired\". Hence, the final context representation will be focusing on all the words **\"the\", \"animal\" and \"tired\"**, and thus it is a superior representation as compared to the traditional way."],"metadata":{"id":"pOHCIzMuVFpl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"32Ni_bJGHpDy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666991841807,"user_tz":180,"elapsed":12,"user":{"displayName":"Edilene Santiago","userId":"06530542736679196152"}},"outputId":"7a9a8e37-a7a0-4554-98cf-fb514cfa77ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Words:\n"," [[1 0 0]\n"," [0 1 0]\n"," [1 1 0]\n"," [0 0 1]] \n"," Shape: (4, 3)\n","\n","\n"]}],"source":["from numpy import array\n","from numpy import random\n","from numpy import dot\n","from scipy.special import softmax\n"," \n","# encoder representations of four different words. Thus, these come from the encoder.\n","word_1 = array([1, 0, 0])\n","word_2 = array([0, 1, 0])\n","word_3 = array([1, 1, 0])\n","word_4 = array([0, 0, 1])\n"," \n","# stacking the word embeddings into a single array\n","words = array([word_1, word_2, word_3, word_4])\n","print('Words:\\n {} \\n Shape: {}\\n\\n'.format(words,words.shape)) \n"]},{"cell_type":"markdown","source":["The next step generates the weight matrices, which you will eventually multiply to the word embeddings to generate the queries, keys, and values. Here, you shall generate these weight matrices randomly; however, in actual practice, these would have been learned during training."],"metadata":{"id":"4eY6He-cuCRr"}},{"cell_type":"code","source":["# generating the weight matrices\n","random.seed(42)\n","W_Q = random.randint(3, size=(3, 3))\n","W_K = random.randint(3, size=(3, 3))\n","W_V = random.randint(3, size=(3, 3))\n","\n","print('Weight matrix - Queries:\\n {} \\n Shape: {}\\n\\n'.format(W_Q,W_Q.shape))\n","print('Weight matrix - Keyes:\\n {} \\n Shape: {}\\n\\n'.format(W_K,W_K.shape))\n","print('Weight matrix - Values:\\n {} \\n Shape: {}\\n\\n'.format(W_V,W_V.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJGLeMaRo_KA","executionInfo":{"status":"ok","timestamp":1666991841807,"user_tz":180,"elapsed":8,"user":{"displayName":"Edilene Santiago","userId":"06530542736679196152"}},"outputId":"cf43d7c9-c984-496d-dbff-9cba5b0f7c36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Weight matrix - Queries:\n"," [[2 0 2]\n"," [2 0 0]\n"," [2 1 2]] \n"," Shape: (3, 3)\n","\n","\n","Weight matrix - Keyes:\n"," [[2 2 2]\n"," [0 2 1]\n"," [0 1 1]] \n"," Shape: (3, 3)\n","\n","\n","Weight matrix - Values:\n"," [[1 1 0]\n"," [0 1 1]\n"," [0 0 0]] \n"," Shape: (3, 3)\n","\n","\n"]}]},{"cell_type":"markdown","source":["Subsequently, the query, key, and value vectors for each word are generated by multiplying each word embedding by each of the weight matrices."],"metadata":{"id":"wT2rg5WmuRfH"}},{"cell_type":"code","source":["# generating the queries, keys and values.\n","# PS: In Python, @ is a binary operator used for matrix multiplication.\n","\n","Q = words @ W_Q\n","K = words @ W_K\n","V = words @ W_V\n","\n","print('Queries:\\n {} \\n Shape: {}\\n\\n'.format(Q,Q.shape))\n","print('Keyes:\\n {} \\n Shape: {}\\n\\n'.format(K,K.shape))\n","print('Values:\\n {} \\n Shape: {}\\n\\n'.format(V,V.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j8pBk2EdpEHA","executionInfo":{"status":"ok","timestamp":1666991841808,"user_tz":180,"elapsed":7,"user":{"displayName":"Edilene Santiago","userId":"06530542736679196152"}},"outputId":"63559c95-f9c8-430e-f5d2-1b3d9d39a0d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Queries:\n"," [[2 0 2]\n"," [2 0 0]\n"," [4 0 2]\n"," [2 1 2]] \n"," Shape: (4, 3)\n","\n","\n","Keyes:\n"," [[2 2 2]\n"," [0 2 1]\n"," [2 4 3]\n"," [0 1 1]] \n"," Shape: (4, 3)\n","\n","\n","Values:\n"," [[1 1 0]\n"," [0 1 1]\n"," [1 2 1]\n"," [0 0 0]] \n"," Shape: (4, 3)\n","\n","\n"]}]},{"cell_type":"markdown","source":["The score values are subsequently passed through a softmax operation to generate the weights. Before doing so, it is common practice to divide the score values by the square root of the dimensionality of the key vectors (in this case, three) to keep the gradients stable."],"metadata":{"id":"r6KA_ZcluhC6"}},{"cell_type":"code","source":["# scoring the query vectors against all key vectors\n","scores = Q @ K.transpose()\n"," \n","# computing the weights by a softmax operation\n","weights = softmax(scores / K.shape[1] ** 0.5, axis=1)\n"," \n","# computing the attention by a weighted sum of the value vectors\n","attention = weights @ V\n"," \n","print('Attention:\\n', attention)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WnrXPeYwpJmF","executionInfo":{"status":"ok","timestamp":1666991841808,"user_tz":180,"elapsed":6,"user":{"displayName":"Edilene Santiago","userId":"06530542736679196152"}},"outputId":"410970b2-be4e-41ad-81aa-2d1df7c28569"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Attention:\n"," [[0.98522025 1.74174051 0.75652026]\n"," [0.90965265 1.40965265 0.5       ]\n"," [0.99851226 1.75849334 0.75998108]\n"," [0.99560386 1.90407309 0.90846923]]\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[{"file_id":"https://github.com/microsoft/nni/blob/ec5f3e63257474c7aee6dcfe7827deb30b7983e3/docs/source/tutorials/hello_nas.ipynb","timestamp":1665761500680}]}},"nbformat":4,"nbformat_minor":0}