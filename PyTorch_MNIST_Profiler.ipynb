{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch MNIST_Valdivino Santiago_Profiling.ipynb","provenance":[{"file_id":"https://github.com/AvivSham/Pytorch-MNIST-colab/blob/master/Pytorch_MNIST.ipynb","timestamp":1631641874508}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"hFJ4A6-l8kwK"},"source":["## **Deep Learning**\n","\n","----\n","\n","**Author**:  <a href=\"https://www.linkedin.com/in/valdivino-alexandre-de-santiago-j%C3%BAnior-103109206/?locale=en_US\">Valdivino Alexandre de Santiago JÃºnior</a>\n","\n","\\\\\n","\n","**Licence**: GNU GENERAL PUBLIC LICENSE, Version 3 (GPLv3)\n","\n","\\\\\n","\n","This notebook is related to the handwritten digit classification problem based on the classical Modified National Institute of Standards and Technology (MNIST) database. Hovewer, its goal is to address performance bottlenecks of the network/model via the TensorBoard Plugin with PyTorch Profiler. Hence, it will not cover the classification task completely. It uses three neural networks: ```SNN500``` by <a href=\"https://github.com/AvivSham/Pytorch-MNIST-colab\">Aviv Shamsian</a>, ```CNN3L``` by <a href=\"https://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118\">Nutan</a>, and ```LeNet-5``` by <a href=\"https://github.com/bollakarthikeya/LeNet-5-PyTorch/blob/master/lenet5_gpu.py\">Bolla Karthikeya</a>. \n","\n","\n"]},{"cell_type":"code","metadata":{"cellView":"code","id":"bGU6NwlsXFSt","executionInfo":{"status":"ok","timestamp":1632775165794,"user_tz":180,"elapsed":794,"user":{"displayName":"Edilene Santiago","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXO-e0pBPOAtncXhtRM-pSuAcG4cE_NmRHk1nvhg=s64","userId":"06530542736679196152"}}},"source":["import torch\n","import torch.nn as nn\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","from prettytable import PrettyTable\n","import matplotlib.pyplot as plt\n","import time\n","import torch.profiler\n","import torch.utils.data"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"vOp6EDP_2yU5","executionInfo":{"status":"ok","timestamp":1632775165794,"user_tz":180,"elapsed":11,"user":{"displayName":"Edilene Santiago","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXO-e0pBPOAtncXhtRM-pSuAcG4cE_NmRHk1nvhg=s64","userId":"06530542736679196152"}}},"source":["# This function obtains the number of trainable parameters of the \n","# model/network.\n","def count_parameters(model):\n","    table = PrettyTable([\"Modules\", \"Parameters\"])\n","    total_params = 0\n","    for name, parameter in model.named_parameters():\n","        if not parameter.requires_grad: continue\n","        param = parameter.numel()\n","        table.add_row([name, param])\n","        total_params+=param\n","    print(table)\n","    print(f\"Total trainable params: {total_params}\")\n","    return total_params"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"kOMpBMxWfCWu","executionInfo":{"status":"ok","timestamp":1632775165795,"user_tz":180,"elapsed":11,"user":{"displayName":"Edilene Santiago","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXO-e0pBPOAtncXhtRM-pSuAcG4cE_NmRHk1nvhg=s64","userId":"06530542736679196152"}}},"source":["# Just visualising some images\n","def visualise_images(img, lab, t):\n","   fig = plt.figure()\n","   for i in range(6):\n","     plt.subplot(2,3,i+1)\n","     plt.tight_layout()\n","     plt.imshow(img[i][0], cmap='gray', interpolation='none')\n","     plt.title(\"{} - class: {}\".format(t,lab[i]))\n","     plt.xticks([])\n","     plt.yticks([])"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MG0mBCsKj5Jv"},"source":["## **Define important variables**\n","----\n","\n","Here, we define the number of classes, some hyper-parameters, and the number of images of the training dataset to consider in the performance analysis."]},{"cell_type":"code","metadata":{"cellView":"code","id":"_bNfVLRUYqZA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632775165795,"user_tz":180,"elapsed":11,"user":{"displayName":"Edilene Santiago","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXO-e0pBPOAtncXhtRM-pSuAcG4cE_NmRHk1nvhg=s64","userId":"06530542736679196152"}},"outputId":"ad71652e-e208-4129-b40d-14959721f6e2"},"source":["num_classes = 10 # Number of output classes, discrete range [0,9]\n","\n","# Hyper-parameters\n","batch_size = 100 # The size of input data took for one iteration (orig = 100)\n","lr = 1e-3 # Learning rate\n","\n","# Number of images: profiling\n","num_images = 10000 # Number of images to consider\n","max_batches = num_images/batch_size # Maximum number of batches\n","print('Maximum number of batches: ', int(max_batches))"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum number of batches:  625\n"]}]},{"cell_type":"markdown","metadata":{"id":"_TypWsyHkQUQ"},"source":["## **MNIST dataset**\n","----\n","\n","Downloading and handling the MNIST dataset. Note that, as the PyTorch documentation, <a href=\"https://pytorch.org/vision/stable/transforms.html\">```transforms.ToTensor()```</a> \"converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8\"."]},{"cell_type":"code","metadata":{"cellView":"code","id":"lCsBCXMwbpH5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632775165795,"user_tz":180,"elapsed":9,"user":{"displayName":"Edilene Santiago","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXO-e0pBPOAtncXhtRM-pSuAcG4cE_NmRHk1nvhg=s64","userId":"06530542736679196152"}},"outputId":"fb2a5d6e-8280-40d5-b606-a7af66995654"},"source":["# Downloading MNIST dataset. We only need the training dataset in this analysis.\n","train_data = dsets.MNIST(root = './data', train = True,\n","                        transform = transforms.ToTensor(), download = True)\n","\n","print('#'*20)\n","print('Training dataset: ', train_data)\n","#print('Test dataset: ', test_data)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["####################\n","Training dataset:  Dataset MNIST\n","    Number of datapoints: 60000\n","    Root location: ./data\n","    Split: Train\n","    StandardTransform\n","Transform: ToTensor()\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n","  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"]}]},{"cell_type":"code","metadata":{"cellView":"code","id":"rfDPBdnYgfGp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632775165796,"user_tz":180,"elapsed":8,"user":{"displayName":"Edilene Santiago","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXO-e0pBPOAtncXhtRM-pSuAcG4cE_NmRHk1nvhg=s64","userId":"06530542736679196152"}},"outputId":"d8d5c1ce-ad9b-4c2e-a92a-e5aaa1cb6884"},"source":["# Wrap an iterable around the dataset to enable easy access to the samples.\n","train_loader = torch.utils.data.DataLoader(dataset = train_data,\n","                                        batch_size = batch_size,\n","                                        shuffle = True)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print('Device is: ', device)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Device is:  cuda:0\n"]}]},{"cell_type":"markdown","metadata":{"id":"-RFrcWLKG3kq"},"source":["## **Looking at the training dataset**\n","\n","----\n","\n","Just taking a quick look at the training dataset.\n","\n"]},{"cell_type":"code","metadata":{"id":"0UTIJs7PFhnZ","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1632775166726,"user_tz":180,"elapsed":935,"user":{"displayName":"Edilene Santiago","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXO-e0pBPOAtncXhtRM-pSuAcG4cE_NmRHk1nvhg=s64","userId":"06530542736679196152"}},"outputId":"d0719601-4e6b-40d0-8a92-a691cab3c03f"},"source":["batch_train = enumerate(train_loader)\n","batch_idx, (batch_train_data, batch_train_classes) = next(batch_train)\n","print('One batch - training dataset:', batch_train_data.shape)\n","\n","print('\\nEach image of the batch:')\n","for i in range(batch_train_classes.shape[0]):\n","  print('Image: {} - Input shape: {} - Class: {}'.format(i, batch_train_data[i].shape, batch_train_classes[i]))\n","  if i == (batch_train_classes.shape[0]-1):\n","    print('The \"image\" itself: ', batch_train_data[i])\n","\n","visualise_images(batch_train_data, batch_train_classes, 'Training')"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["One batch - training dataset: torch.Size([16, 1, 28, 28])\n","\n","Each image of the batch:\n","Image: 0 - Input shape: torch.Size([1, 28, 28]) - Class: 4\n","Image: 1 - Input shape: torch.Size([1, 28, 28]) - Class: 3\n","Image: 2 - Input shape: torch.Size([1, 28, 28]) - Class: 1\n","Image: 3 - Input shape: torch.Size([1, 28, 28]) - Class: 2\n","Image: 4 - Input shape: torch.Size([1, 28, 28]) - Class: 9\n","Image: 5 - Input shape: torch.Size([1, 28, 28]) - Class: 9\n","Image: 6 - Input shape: torch.Size([1, 28, 28]) - Class: 1\n","Image: 7 - Input shape: torch.Size([1, 28, 28]) - Class: 1\n","Image: 8 - Input shape: torch.Size([1, 28, 28]) - Class: 4\n","Image: 9 - Input shape: torch.Size([1, 28, 28]) - Class: 4\n","Image: 10 - Input shape: torch.Size([1, 28, 28]) - Class: 5\n","Image: 11 - Input shape: torch.Size([1, 28, 28]) - Class: 6\n","Image: 12 - Input shape: torch.Size([1, 28, 28]) - Class: 5\n","Image: 13 - Input shape: torch.Size([1, 28, 28]) - Class: 6\n","Image: 14 - Input shape: torch.Size([1, 28, 28]) - Class: 1\n","Image: 15 - Input shape: torch.Size([1, 28, 28]) - Class: 6\n","The \"image\" itself:  tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.3255, 0.6784, 0.0588, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.2157, 0.2314, 0.0902, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.2667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0078, 0.4627, 0.9804, 0.9922, 0.5020, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.1961, 0.9922, 0.9412, 0.3922, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4235,\n","          0.9529, 0.9412, 0.1216, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1490, 0.9059,\n","          0.9608, 0.2745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4745, 1.0000,\n","          0.8275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3098, 0.9569, 0.9765,\n","          0.3451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0980, 0.8235, 0.9922, 0.6471,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.6588, 0.9922, 0.9294, 0.1765,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0314, 0.8039, 0.9922, 0.4627, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.5490, 0.9961, 0.8118, 0.0902, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0196, 0.8510, 0.9922, 0.4275, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.2314, 0.9922, 0.6588, 0.0235, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.2314, 0.9922, 0.3059, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.5255, 0.9922, 0.3059, 0.0000, 0.1490, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.6118, 0.9961, 0.3765, 0.6431, 0.9961, 0.9098,\n","          0.3294, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.2706, 0.9804, 0.9333, 0.9922, 0.9922, 0.9961,\n","          0.8549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.8392, 0.9922, 0.9922, 0.9922, 0.9961,\n","          0.6471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.8392, 0.9922, 0.9922, 0.9922, 0.8118,\n","          0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.8392, 0.9922, 0.9294, 0.4627, 0.0902,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000]]])\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 6 Axes>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"cWqmSTIo_yYv"},"source":["## **Neural networks**\n","\n","----\n","\n","We use three neural networks in this analysis. The first is ```SNN500``` which is a shallow neural network (SNN) with a single hidden layer containing 500 neurons (traditionally, a model/network is considered shallow when it has one or two hidden layers). \n","\n","<img src=\"https://scipython.com/static/media/uploads/blog/shallow-neural-net/snn.png\" width=\"300\"/>\n","\n","\\\\\n","\n","The second network is ```CNN3L``` which is also considered an SNN because it has two hidden (convolutional) layers and the output layer \n","\n","\\\\\n","\n","The third network is the classical <a href=\"https://ieeexplore.ieee.org/document/726791\">```LeNet-5```</a> DNN with five layers where four are hidden ones.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"cellView":"code","id":"fL-YXTvghaz_","executionInfo":{"status":"ok","timestamp":1632775166726,"user_tz":180,"elapsed":10,"user":{"displayName":"Edilene Santiago","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXO-e0pBPOAtncXhtRM-pSuAcG4cE_NmRHk1nvhg=s64","userId":"06530542736679196152"}}},"source":["class SNN500(nn.Module):\n","  def __init__(self, input_sz, hidden_sz, num_clas):\n","    super(SNN500,self).__init__()\n","    self.fc1 = nn.Linear(input_sz, hidden_sz)\n","    self.relu = nn.ReLU()\n","    self.fc2 = nn.Linear(hidden_sz, num_clas)\n","  \n","  def forward(self,x):\n","    out = self.fc1(x)\n","    out = self.relu(out)\n","    out = self.fc2(out)\n","    return out"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"78aQsuXG2HMl","executionInfo":{"status":"ok","timestamp":1632775166727,"user_tz":180,"elapsed":11,"user":{"displayName":"Edilene Santiago","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXO-e0pBPOAtncXhtRM-pSuAcG4cE_NmRHk1nvhg=s64","userId":"06530542736679196152"}}},"source":["class CNN3L(nn.Module):\n","    def __init__(self, num_clas):\n","        super(CNN3L, self).__init__()\n","        self.conv1 = nn.Sequential(         \n","            nn.Conv2d(\n","                in_channels=1,              \n","                out_channels=16,            \n","                kernel_size=5,              \n","                stride=1,                   \n","                padding=2,                  \n","            ),                              \n","            nn.ReLU(),                      \n","            nn.MaxPool2d(kernel_size=2),    \n","        )\n","        self.conv2 = nn.Sequential(         \n","            nn.Conv2d(16, 32, 5, 1, 2),     \n","            nn.ReLU(),                      \n","            nn.MaxPool2d(kernel_size=2),                \n","        )\n","        # Fully-connected layer\n","        self.out = nn.Linear(32 * 7 * 7, num_clas)\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        # Flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n","        x = x.view(x.size(0), -1)       \n","        output = self.out(x)\n","        return output   "],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"fzqclRyR-XPU","executionInfo":{"status":"ok","timestamp":1632775166727,"user_tz":180,"elapsed":10,"user":{"displayName":"Edilene Santiago","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXO-e0pBPOAtncXhtRM-pSuAcG4cE_NmRHk1nvhg=s64","userId":"06530542736679196152"}}},"source":["class LeNet5(nn.Module):          \n","    def __init__(self, num_clas):     \n","        super(LeNet5, self).__init__()\n","        # Convolution \n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2, bias=True)\n","        # Max-pooling\n","        self.max_pool_1 = nn.MaxPool2d(kernel_size=2)\n","        # Convolution\n","        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)\n","        # Max-pooling\n","        self.max_pool_2 = nn.MaxPool2d(kernel_size=2) \n","        # Fully-connected layers\n","        self.fc1 = nn.Linear(16*5*5, 120)   # convert matrix with 16*5*5 (= 400) features to a matrix of 120 features (columns)\n","        self.fc2 = nn.Linear(120, 84)       # convert matrix with 120 features to a matrix of 84 features (columns)\n","        self.fc3 = nn.Linear(84, num_clas)        # convert matrix with 84 features to a matrix of 10 features (columns)\n","        \n","    def forward(self, x):\n","        # Convolve, then perform ReLU non-linearity\n","        x = nn.functional.relu(self.conv1(x))  \n","        # Max-pooling with 2x2 grid \n","        x = self.max_pool_1(x) \n","        # Convolve, then perform ReLU non-linearity\n","        x = nn.functional.relu(self.conv2(x))\n","        # Max-pooling with 2x2 grid\n","        x = self.max_pool_2(x)\n","        # First flatten 'max_pool_2_out' to contain 16*5*5 columns\n","        x = x.view(-1, 16*5*5)\n","        # FC-1, then perform ReLU non-linearity\n","        x = nn.functional.relu(self.fc1(x))\n","        # FC-2, then perform ReLU non-linearity\n","        x = nn.functional.relu(self.fc2(x))\n","        # FC-3\n","        x = self.fc3(x)\n","        return x"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6DPf390KCQta"},"source":["## **Select the model**\n","\n","----\n","\n","Now, we can select one of the models/networks."]},{"cell_type":"code","metadata":{"cellView":"code","id":"-3EPEqbjjfAT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632775173054,"user_tz":180,"elapsed":6337,"user":{"displayName":"Edilene Santiago","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXO-e0pBPOAtncXhtRM-pSuAcG4cE_NmRHk1nvhg=s64","userId":"06530542736679196152"}},"outputId":"cb616172-bf3f-4beb-e973-1daf127d450b"},"source":["opt = input(\"Enter your choice:\")\n","if opt =='1':\n","  input_size = 784 # img_size = (28,28) ---> 28*28=784 in total\n","  hidden_size_tb = 500 # Number of nodes at hidden layer\n","  net = SNN500(input_size, hidden_size_tb, num_classes)\n","  opt_name = 'SNN500'\n","  print(\"You selected SNN500!\")\n","elif opt =='2':\n","  net = CNN3L(num_classes)\n","  opt_name = 'CNN3L'\n","  print(\"You selected CNN3L!\")\n","elif opt =='3':\n","  net = LeNet5(num_classes)\n","  opt_name = 'LeNet-5'\n","  print(\"You selected LeNet-5!\")\n","else:\n","  print(\"Invalid Option!\")\n","\n","if torch.cuda.is_available():\n","   net.to(device)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter your choice:3\n","You selected LeNet-5!\n"]}]},{"cell_type":"markdown","metadata":{"id":"syNIW-CMChqO"},"source":["## **Define loss function and optimiser**\n","\n","----\n","\n","The <a href=\"https://github.com/AvivSham/Pytorch-MNIST-colab\">CrossEntropyLoss</a> function combines LogSoftmax and NLLLoss in one single class. The learning rate has already been defined."]},{"cell_type":"code","metadata":{"cellView":"code","id":"ePLIwvAFj2zH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632775173056,"user_tz":180,"elapsed":15,"user":{"displayName":"Edilene Santiago","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXO-e0pBPOAtncXhtRM-pSuAcG4cE_NmRHk1nvhg=s64","userId":"06530542736679196152"}},"outputId":"0bc6e2d7-dc76-409d-b3a4-dcff72797fdc"},"source":["loss_function = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","\n","print('Checking trainable parameters: {}'.format(count_parameters(net)))"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------+------------+\n","|   Modules    | Parameters |\n","+--------------+------------+\n","| conv1.weight |    150     |\n","|  conv1.bias  |     6      |\n","| conv2.weight |    2400    |\n","|  conv2.bias  |     16     |\n","|  fc1.weight  |   48000    |\n","|   fc1.bias   |    120     |\n","|  fc2.weight  |   10080    |\n","|   fc2.bias   |     84     |\n","|  fc3.weight  |    840     |\n","|   fc3.bias   |     10     |\n","+--------------+------------+\n","Total trainable params: 61706\n","Checking trainable parameters: 61706\n"]}]},{"cell_type":"markdown","metadata":{"id":"-9_YTPhfCzkk"},"source":["## **Training phase**\n","\n","----\n","\n","Here, we starting training the model (```net.train()``` and function ```train_model(data)```)."]},{"cell_type":"code","metadata":{"id":"s4sqav0i78RG","executionInfo":{"status":"ok","timestamp":1632775173056,"user_tz":180,"elapsed":13,"user":{"displayName":"Edilene Santiago","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXO-e0pBPOAtncXhtRM-pSuAcG4cE_NmRHk1nvhg=s64","userId":"06530542736679196152"}}},"source":["net.train() \n","def train_model(data):\n","  if opt == '1':\n","    images, labels = Variable(data[0].view(-1,28*28)).to(device), Variable(data[1]).to(device)\n","  elif (opt == '2') or (opt == '3'):\n","    images, labels = Variable(data[0]).to(device), Variable(data[1]).to(device) \n","      \n","  optimizer.zero_grad()\n","  with torch.set_grad_enabled(True):\n","    outputs = net(images)\n","    _, preds = torch.max(outputs, 1)\n","    loss = loss_function(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HJoq5eWj3EvI"},"source":["## **Use the PyTorch Profiler**\n","----\n","\n","Here, we use the PyTorch Profiler to record execution events. After running this notebook, the results will be saved under ```./log/xyz``` folder."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5j-I77pL8AEz","executionInfo":{"status":"ok","timestamp":1632775185111,"user_tz":180,"elapsed":12067,"user":{"displayName":"Edilene Santiago","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXO-e0pBPOAtncXhtRM-pSuAcG4cE_NmRHk1nvhg=s64","userId":"06530542736679196152"}},"outputId":"86993b0e-c8df-4b7f-daef-4b3dd570b7f2"},"source":["with torch.profiler.profile(\n","        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n","        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/mnist_'+opt_name+'_bat_'+str(batch_size)),\n","        record_shapes=True,\n","        with_stack=True\n",") as prof:\n","    for num_batch, batch_data in enumerate(train_loader):\n","        print('Batch number:', num_batch)\n","        if num_batch >= int(max_batches): \n","            break\n","        train_model(batch_data)\n","        prof.step()  # Need to call this at the end of each step to notify profiler of steps' boundary."],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch number: 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]},{"output_type":"stream","name":"stdout","text":["Batch number: 1\n","Batch number: 2\n","Batch number: 3\n","Batch number: 4\n","Batch number: 5\n","Batch number: 6\n","Batch number: 7\n","Batch number: 8\n","Batch number: 9\n","Batch number: 10\n","Batch number: 11\n","Batch number: 12\n","Batch number: 13\n","Batch number: 14\n","Batch number: 15\n","Batch number: 16\n","Batch number: 17\n","Batch number: 18\n","Batch number: 19\n","Batch number: 20\n","Batch number: 21\n","Batch number: 22\n","Batch number: 23\n","Batch number: 24\n","Batch number: 25\n","Batch number: 26\n","Batch number: 27\n","Batch number: 28\n","Batch number: 29\n","Batch number: 30\n","Batch number: 31\n","Batch number: 32\n","Batch number: 33\n","Batch number: 34\n","Batch number: 35\n","Batch number: 36\n","Batch number: 37\n","Batch number: 38\n","Batch number: 39\n","Batch number: 40\n","Batch number: 41\n","Batch number: 42\n","Batch number: 43\n","Batch number: 44\n","Batch number: 45\n","Batch number: 46\n","Batch number: 47\n","Batch number: 48\n","Batch number: 49\n","Batch number: 50\n","Batch number: 51\n","Batch number: 52\n","Batch number: 53\n","Batch number: 54\n","Batch number: 55\n","Batch number: 56\n","Batch number: 57\n","Batch number: 58\n","Batch number: 59\n","Batch number: 60\n","Batch number: 61\n","Batch number: 62\n","Batch number: 63\n","Batch number: 64\n","Batch number: 65\n","Batch number: 66\n","Batch number: 67\n","Batch number: 68\n","Batch number: 69\n","Batch number: 70\n","Batch number: 71\n","Batch number: 72\n","Batch number: 73\n","Batch number: 74\n","Batch number: 75\n","Batch number: 76\n","Batch number: 77\n","Batch number: 78\n","Batch number: 79\n","Batch number: 80\n","Batch number: 81\n","Batch number: 82\n","Batch number: 83\n","Batch number: 84\n","Batch number: 85\n","Batch number: 86\n","Batch number: 87\n","Batch number: 88\n","Batch number: 89\n","Batch number: 90\n","Batch number: 91\n","Batch number: 92\n","Batch number: 93\n","Batch number: 94\n","Batch number: 95\n","Batch number: 96\n","Batch number: 97\n","Batch number: 98\n","Batch number: 99\n","Batch number: 100\n","Batch number: 101\n","Batch number: 102\n","Batch number: 103\n","Batch number: 104\n","Batch number: 105\n","Batch number: 106\n","Batch number: 107\n","Batch number: 108\n","Batch number: 109\n","Batch number: 110\n","Batch number: 111\n","Batch number: 112\n","Batch number: 113\n","Batch number: 114\n","Batch number: 115\n","Batch number: 116\n","Batch number: 117\n","Batch number: 118\n","Batch number: 119\n","Batch number: 120\n","Batch number: 121\n","Batch number: 122\n","Batch number: 123\n","Batch number: 124\n","Batch number: 125\n","Batch number: 126\n","Batch number: 127\n","Batch number: 128\n","Batch number: 129\n","Batch number: 130\n","Batch number: 131\n","Batch number: 132\n","Batch number: 133\n","Batch number: 134\n","Batch number: 135\n","Batch number: 136\n","Batch number: 137\n","Batch number: 138\n","Batch number: 139\n","Batch number: 140\n","Batch number: 141\n","Batch number: 142\n","Batch number: 143\n","Batch number: 144\n","Batch number: 145\n","Batch number: 146\n","Batch number: 147\n","Batch number: 148\n","Batch number: 149\n","Batch number: 150\n","Batch number: 151\n","Batch number: 152\n","Batch number: 153\n","Batch number: 154\n","Batch number: 155\n","Batch number: 156\n","Batch number: 157\n","Batch number: 158\n","Batch number: 159\n","Batch number: 160\n","Batch number: 161\n","Batch number: 162\n","Batch number: 163\n","Batch number: 164\n","Batch number: 165\n","Batch number: 166\n","Batch number: 167\n","Batch number: 168\n","Batch number: 169\n","Batch number: 170\n","Batch number: 171\n","Batch number: 172\n","Batch number: 173\n","Batch number: 174\n","Batch number: 175\n","Batch number: 176\n","Batch number: 177\n","Batch number: 178\n","Batch number: 179\n","Batch number: 180\n","Batch number: 181\n","Batch number: 182\n","Batch number: 183\n","Batch number: 184\n","Batch number: 185\n","Batch number: 186\n","Batch number: 187\n","Batch number: 188\n","Batch number: 189\n","Batch number: 190\n","Batch number: 191\n","Batch number: 192\n","Batch number: 193\n","Batch number: 194\n","Batch number: 195\n","Batch number: 196\n","Batch number: 197\n","Batch number: 198\n","Batch number: 199\n","Batch number: 200\n","Batch number: 201\n","Batch number: 202\n","Batch number: 203\n","Batch number: 204\n","Batch number: 205\n","Batch number: 206\n","Batch number: 207\n","Batch number: 208\n","Batch number: 209\n","Batch number: 210\n","Batch number: 211\n","Batch number: 212\n","Batch number: 213\n","Batch number: 214\n","Batch number: 215\n","Batch number: 216\n","Batch number: 217\n","Batch number: 218\n","Batch number: 219\n","Batch number: 220\n","Batch number: 221\n","Batch number: 222\n","Batch number: 223\n","Batch number: 224\n","Batch number: 225\n","Batch number: 226\n","Batch number: 227\n","Batch number: 228\n","Batch number: 229\n","Batch number: 230\n","Batch number: 231\n","Batch number: 232\n","Batch number: 233\n","Batch number: 234\n","Batch number: 235\n","Batch number: 236\n","Batch number: 237\n","Batch number: 238\n","Batch number: 239\n","Batch number: 240\n","Batch number: 241\n","Batch number: 242\n","Batch number: 243\n","Batch number: 244\n","Batch number: 245\n","Batch number: 246\n","Batch number: 247\n","Batch number: 248\n","Batch number: 249\n","Batch number: 250\n","Batch number: 251\n","Batch number: 252\n","Batch number: 253\n","Batch number: 254\n","Batch number: 255\n","Batch number: 256\n","Batch number: 257\n","Batch number: 258\n","Batch number: 259\n","Batch number: 260\n","Batch number: 261\n","Batch number: 262\n","Batch number: 263\n","Batch number: 264\n","Batch number: 265\n","Batch number: 266\n","Batch number: 267\n","Batch number: 268\n","Batch number: 269\n","Batch number: 270\n","Batch number: 271\n","Batch number: 272\n","Batch number: 273\n","Batch number: 274\n","Batch number: 275\n","Batch number: 276\n","Batch number: 277\n","Batch number: 278\n","Batch number: 279\n","Batch number: 280\n","Batch number: 281\n","Batch number: 282\n","Batch number: 283\n","Batch number: 284\n","Batch number: 285\n","Batch number: 286\n","Batch number: 287\n","Batch number: 288\n","Batch number: 289\n","Batch number: 290\n","Batch number: 291\n","Batch number: 292\n","Batch number: 293\n","Batch number: 294\n","Batch number: 295\n","Batch number: 296\n","Batch number: 297\n","Batch number: 298\n","Batch number: 299\n","Batch number: 300\n","Batch number: 301\n","Batch number: 302\n","Batch number: 303\n","Batch number: 304\n","Batch number: 305\n","Batch number: 306\n","Batch number: 307\n","Batch number: 308\n","Batch number: 309\n","Batch number: 310\n","Batch number: 311\n","Batch number: 312\n","Batch number: 313\n","Batch number: 314\n","Batch number: 315\n","Batch number: 316\n","Batch number: 317\n","Batch number: 318\n","Batch number: 319\n","Batch number: 320\n","Batch number: 321\n","Batch number: 322\n","Batch number: 323\n","Batch number: 324\n","Batch number: 325\n","Batch number: 326\n","Batch number: 327\n","Batch number: 328\n","Batch number: 329\n","Batch number: 330\n","Batch number: 331\n","Batch number: 332\n","Batch number: 333\n","Batch number: 334\n","Batch number: 335\n","Batch number: 336\n","Batch number: 337\n","Batch number: 338\n","Batch number: 339\n","Batch number: 340\n","Batch number: 341\n","Batch number: 342\n","Batch number: 343\n","Batch number: 344\n","Batch number: 345\n","Batch number: 346\n","Batch number: 347\n","Batch number: 348\n","Batch number: 349\n","Batch number: 350\n","Batch number: 351\n","Batch number: 352\n","Batch number: 353\n","Batch number: 354\n","Batch number: 355\n","Batch number: 356\n","Batch number: 357\n","Batch number: 358\n","Batch number: 359\n","Batch number: 360\n","Batch number: 361\n","Batch number: 362\n","Batch number: 363\n","Batch number: 364\n","Batch number: 365\n","Batch number: 366\n","Batch number: 367\n","Batch number: 368\n","Batch number: 369\n","Batch number: 370\n","Batch number: 371\n","Batch number: 372\n","Batch number: 373\n","Batch number: 374\n","Batch number: 375\n","Batch number: 376\n","Batch number: 377\n","Batch number: 378\n","Batch number: 379\n","Batch number: 380\n","Batch number: 381\n","Batch number: 382\n","Batch number: 383\n","Batch number: 384\n","Batch number: 385\n","Batch number: 386\n","Batch number: 387\n","Batch number: 388\n","Batch number: 389\n","Batch number: 390\n","Batch number: 391\n","Batch number: 392\n","Batch number: 393\n","Batch number: 394\n","Batch number: 395\n","Batch number: 396\n","Batch number: 397\n","Batch number: 398\n","Batch number: 399\n","Batch number: 400\n","Batch number: 401\n","Batch number: 402\n","Batch number: 403\n","Batch number: 404\n","Batch number: 405\n","Batch number: 406\n","Batch number: 407\n","Batch number: 408\n","Batch number: 409\n","Batch number: 410\n","Batch number: 411\n","Batch number: 412\n","Batch number: 413\n","Batch number: 414\n","Batch number: 415\n","Batch number: 416\n","Batch number: 417\n","Batch number: 418\n","Batch number: 419\n","Batch number: 420\n","Batch number: 421\n","Batch number: 422\n","Batch number: 423\n","Batch number: 424\n","Batch number: 425\n","Batch number: 426\n","Batch number: 427\n","Batch number: 428\n","Batch number: 429\n","Batch number: 430\n","Batch number: 431\n","Batch number: 432\n","Batch number: 433\n","Batch number: 434\n","Batch number: 435\n","Batch number: 436\n","Batch number: 437\n","Batch number: 438\n","Batch number: 439\n","Batch number: 440\n","Batch number: 441\n","Batch number: 442\n","Batch number: 443\n","Batch number: 444\n","Batch number: 445\n","Batch number: 446\n","Batch number: 447\n","Batch number: 448\n","Batch number: 449\n","Batch number: 450\n","Batch number: 451\n","Batch number: 452\n","Batch number: 453\n","Batch number: 454\n","Batch number: 455\n","Batch number: 456\n","Batch number: 457\n","Batch number: 458\n","Batch number: 459\n","Batch number: 460\n","Batch number: 461\n","Batch number: 462\n","Batch number: 463\n","Batch number: 464\n","Batch number: 465\n","Batch number: 466\n","Batch number: 467\n","Batch number: 468\n","Batch number: 469\n","Batch number: 470\n","Batch number: 471\n","Batch number: 472\n","Batch number: 473\n","Batch number: 474\n","Batch number: 475\n","Batch number: 476\n","Batch number: 477\n","Batch number: 478\n","Batch number: 479\n","Batch number: 480\n","Batch number: 481\n","Batch number: 482\n","Batch number: 483\n","Batch number: 484\n","Batch number: 485\n","Batch number: 486\n","Batch number: 487\n","Batch number: 488\n","Batch number: 489\n","Batch number: 490\n","Batch number: 491\n","Batch number: 492\n","Batch number: 493\n","Batch number: 494\n","Batch number: 495\n","Batch number: 496\n","Batch number: 497\n","Batch number: 498\n","Batch number: 499\n","Batch number: 500\n","Batch number: 501\n","Batch number: 502\n","Batch number: 503\n","Batch number: 504\n","Batch number: 505\n","Batch number: 506\n","Batch number: 507\n","Batch number: 508\n","Batch number: 509\n","Batch number: 510\n","Batch number: 511\n","Batch number: 512\n","Batch number: 513\n","Batch number: 514\n","Batch number: 515\n","Batch number: 516\n","Batch number: 517\n","Batch number: 518\n","Batch number: 519\n","Batch number: 520\n","Batch number: 521\n","Batch number: 522\n","Batch number: 523\n","Batch number: 524\n","Batch number: 525\n","Batch number: 526\n","Batch number: 527\n","Batch number: 528\n","Batch number: 529\n","Batch number: 530\n","Batch number: 531\n","Batch number: 532\n","Batch number: 533\n","Batch number: 534\n","Batch number: 535\n","Batch number: 536\n","Batch number: 537\n","Batch number: 538\n","Batch number: 539\n","Batch number: 540\n","Batch number: 541\n","Batch number: 542\n","Batch number: 543\n","Batch number: 544\n","Batch number: 545\n","Batch number: 546\n","Batch number: 547\n","Batch number: 548\n","Batch number: 549\n","Batch number: 550\n","Batch number: 551\n","Batch number: 552\n","Batch number: 553\n","Batch number: 554\n","Batch number: 555\n","Batch number: 556\n","Batch number: 557\n","Batch number: 558\n","Batch number: 559\n","Batch number: 560\n","Batch number: 561\n","Batch number: 562\n","Batch number: 563\n","Batch number: 564\n","Batch number: 565\n","Batch number: 566\n","Batch number: 567\n","Batch number: 568\n","Batch number: 569\n","Batch number: 570\n","Batch number: 571\n","Batch number: 572\n","Batch number: 573\n","Batch number: 574\n","Batch number: 575\n","Batch number: 576\n","Batch number: 577\n","Batch number: 578\n","Batch number: 579\n","Batch number: 580\n","Batch number: 581\n","Batch number: 582\n","Batch number: 583\n","Batch number: 584\n","Batch number: 585\n","Batch number: 586\n","Batch number: 587\n","Batch number: 588\n","Batch number: 589\n","Batch number: 590\n","Batch number: 591\n","Batch number: 592\n","Batch number: 593\n","Batch number: 594\n","Batch number: 595\n","Batch number: 596\n","Batch number: 597\n","Batch number: 598\n","Batch number: 599\n","Batch number: 600\n","Batch number: 601\n","Batch number: 602\n","Batch number: 603\n","Batch number: 604\n","Batch number: 605\n","Batch number: 606\n","Batch number: 607\n","Batch number: 608\n","Batch number: 609\n","Batch number: 610\n","Batch number: 611\n","Batch number: 612\n","Batch number: 613\n","Batch number: 614\n","Batch number: 615\n","Batch number: 616\n","Batch number: 617\n","Batch number: 618\n","Batch number: 619\n","Batch number: 620\n","Batch number: 621\n","Batch number: 622\n","Batch number: 623\n","Batch number: 624\n","Batch number: 625\n"]}]},{"cell_type":"markdown","metadata":{"id":"A7fOQmQO3wT6"},"source":["## **Run TensorBoard**\n","----\n","\n","In order to visualise the results and check the performance bottlenecks of the model, proceed as follows as per the <a href=\"https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html\">PyTorch tutorial </a>:\n","\n","\n","\n","1. download the results in ```./log/xyz``` to your computer; \n","2. install PyTorch Profiler TensorBoard Plugin (```pip install torch_tb_profiler```);\n","3. launch the TensorBoard (```tensorboard --logdir=./log```);\n","4. open the TensorBoard profile URL in Google Chrome browser (```http://localhost:6006/#pytorch_profiler```). \n","\n","\n","\n"]}]}